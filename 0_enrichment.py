# -*- coding: utf-8 -*-
"""enrichment.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1XvbBIWVfmIGScfttfOjgiimGABr9RiPX
"""

# !python -m spacy download en_core_web_sm
# !pip install afinn
# !pip install nltk
# !pip install stanza

# !pip install labMTsimple

from afinn import Afinn
import pandas as pd
import re
from labMTsimple.storyLab import *
import nltk
from nltk.tree import Tree
from nltk.tree import ParentedTree
from nltk.tokenize import sent_tokenize
import numpy as np
import spacy
import stanza

nltk.download('punkt')
nltk.download('averaged_perceptron_tagger')
nltk.download('large_grammars')
stanza.download('en')

reddit_data = pd.read_csv('500_Reddit_users_posts_labels.csv')
reddit_data.head()

"""## Sentiment"""

reddit_data = reddit_data.rename(columns={'User':'user', 'Post':'post', 'Label':'label'})

afn = Afinn()

def get_sentiment(text):
    score = afn.score(text)
    if score > 0:
        return 'positive'
    elif score < 0:
        return 'negative'
    else:
        return 'neutral'

reddit_data['afinn_score'] = reddit_data['post'].apply(afn.score)
reddit_data['sentiment'] = reddit_data['post'].apply(get_sentiment)

"""## Personal Pronoun Count and Ratio"""

def get_pronoun_count(text):
    pronouns = re.findall(r'\b(I|me|mine|myself|we|us|ours|ourselves)\b', text, re.IGNORECASE)
    return len(pronouns)

reddit_data['personal_pronoun_count'] = reddit_data['post'].apply(get_pronoun_count)
reddit_data['personal_pronoun_itra_ratio'] = reddit_data['personal_pronoun_count'] / reddit_data['personal_pronoun_count'].sum()

"""## Number of Sentences"""

nltk.download('punkt')
nltk.download('averaged_perceptron_tagger')

def count_sentences(text):
    sentences = nltk.sent_tokenize(text)
    return len(sentences)

reddit_data['num_sentences'] = reddit_data['post'].apply(count_sentences)

"""## Definite Articles"""

def count_articles(text):
    articles = re.findall(r'\b(the)\b', text, re.IGNORECASE)
    return len(articles)

reddit_data["num_articles"] = reddit_data['post'].apply(count_articles)

"""## Pronouns"""

def count_pronouns(text):
    sentences = nltk.word_tokenize(text)
    tags = nltk.pos_tag(sentences)
    pronouns = ['PRP', 'PRP$', 'WP', 'WP$']
    
    num_pronouns = sum([1 for word, tag in tags if tag in pronouns])
    
    return num_pronouns

reddit_data["num_pronouns"] = reddit_data['post'].apply(count_pronouns)
reddit_data["intra_pronoun_to_avg_ratio"] = reddit_data["num_pronouns"] / np.mean(reddit_data["num_pronouns"])

"""## Tree height """

snlp = spacy.load("en_core_web_sm")

def tree_height(node):
    if not list(node.children):
        return 1
    else:
        return 1 + max(tree_height(child) for child in node.children)

def get_tree_height(text):
    
    doc = snlp(text)
    heights = [tree_height(sent.root) for sent in doc.sents]
    
    if not heights:
        return None, None
    
    return np.mean(heights), max(heights)

reddit_data["mean_tree_height"], reddit_data["max_tree_height"] = zip(*reddit_data['post'].apply(get_tree_height))

"""## LabMT"""

def get_strict_match(text):
    lang = 'english'
    labMT, labMTvector, labMTwordList = emotionFileReader(stopval=0.0,lang=lang, returnVector=True)
    
    textValence, textFvec = emotion(text, labMT, shift=True, happsList=labMTvector)
    textStoppedVec = stopper(textFvec, labMTvector, labMTwordList, stopVal=1.0)
    textValence = emotionV(textStoppedVec, labMTvector)
    return textValence

reddit_data["labmt_valence"] = reddit_data['post'].apply(get_strict_match)
reddit_data["labmt_intra_avg_valence"] = reddit_data['labmt_valence'] / np.mean(reddit_data['labmt_valence'])

"""## Normalization"""

reddit_data_enriched = pd.DataFrame()
reddit_data_enriched['user'] = reddit_data['user']
reddit_data_enriched['post'] = reddit_data['post']
reddit_data_enriched['label'] = reddit_data['label']

def normalize(series):
    return (series - series.mean()) / series.std()

reddit_data_enriched['afinn_score'] = normalize(reddit_data['afinn_score'])
reddit_data_enriched['sentiment'] = reddit_data['sentiment']
reddit_data_enriched = pd.get_dummies(reddit_data_enriched, columns=['sentiment'])
reddit_data_enriched['personal_pronoun_count'] = normalize(reddit_data['personal_pronoun_count'])
reddit_data_enriched['personal_pronoun_itra_ratio'] = normalize(reddit_data['personal_pronoun_itra_ratio'])
reddit_data_enriched['num_sentences'] = normalize(reddit_data['num_sentences'])
reddit_data_enriched['num_articles'] = normalize(reddit_data['num_articles'])
reddit_data_enriched['num_pronouns'] = normalize(reddit_data['num_pronouns'])
reddit_data_enriched['intra_pronoun_to_avg_ratio'] = normalize(reddit_data['intra_pronoun_to_avg_ratio'])
reddit_data_enriched['mean_tree_height'] = normalize(reddit_data['mean_tree_height'])
reddit_data_enriched['max_tree_height'] = normalize(reddit_data['max_tree_height'])
reddit_data_enriched['labmt_valence'] = normalize(reddit_data['labmt_valence'])



reddit_data_enriched.to_csv('reddit_data_with_cf.csv', index=False)

reddit_data_enriched.head()



