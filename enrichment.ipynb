{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "7dfdd581-eba7-4db4-9e03-8453a8485937",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.5.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.5.0/en_core_web_sm-3.5.0-py3-none-any.whl (12.8 MB)\n",
      "     --------------------------------------- 12.8/12.8 MB 18.2 MB/s eta 0:00:00\n",
      "Requirement already satisfied: spacy<3.6.0,>=3.5.0 in c:\\users\\ckhoe\\anaconda3\\lib\\site-packages (from en-core-web-sm==3.5.0) (3.5.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\ckhoe\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.28.1)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in c:\\users\\ckhoe\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.10.7)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\ckhoe\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.0.4)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\ckhoe\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.0.8)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\ckhoe\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.0.12)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\ckhoe\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.23.5)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in c:\\users\\ckhoe\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (8.1.9)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\ckhoe\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.1.1)\n",
      "Requirement already satisfied: pathy>=0.10.0 in c:\\users\\ckhoe\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.10.1)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\ckhoe\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.0.8)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\ckhoe\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.0.9)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\ckhoe\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.3.0)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\ckhoe\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.0.7)\n",
      "Requirement already satisfied: typer<0.8.0,>=0.3.0 in c:\\users\\ckhoe\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.7.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\ckhoe\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (22.0)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\ckhoe\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.4.6)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in c:\\users\\ckhoe\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (5.2.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\ckhoe\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (4.64.1)\n",
      "Requirement already satisfied: setuptools in c:\\users\\ckhoe\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (65.6.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\ckhoe\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.11.3)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in c:\\users\\ckhoe\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (4.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\ckhoe\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2022.12.7)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\ckhoe\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\ckhoe\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\ckhoe\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.26.14)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\ckhoe\\anaconda3\\lib\\site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.0.4)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\users\\ckhoe\\anaconda3\\lib\\site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.7.9)\n",
      "Requirement already satisfied: colorama in c:\\users\\ckhoe\\anaconda3\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.4.6)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\ckhoe\\anaconda3\\lib\\site-packages (from typer<0.8.0,>=0.3.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (8.0.4)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\ckhoe\\anaconda3\\lib\\site-packages (from jinja2->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.0.1)\n",
      "\u001b[38;5;2m[+] Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "40cabb9a-5f8a-4b26-9d27-9174abacff76",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ckhoe\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\ckhoe\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package large_grammars to\n",
      "[nltk_data]     C:\\Users\\ckhoe\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package large_grammars is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from afinn import Afinn\n",
    "import pandas as pd\n",
    "import re\n",
    "from labMTsimple.storyLab import *\n",
    "import nltk\n",
    "from nltk.tree import Tree\n",
    "from nltk.tree import ParentedTree\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import numpy as np\n",
    "import spacy\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('large_grammars')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3429878c-0c34-4e65-bf6b-a743e8be85f7",
   "metadata": {},
   "source": [
    "## Sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "447a63c4-17a9-4cea-97ab-ae088e3cb9a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_data = reddit_data.rename(columns={'User':'user', 'Post':'post', 'Label':'label'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "9a65b523-6691-4a05-8095-11252132ce82",
   "metadata": {},
   "outputs": [],
   "source": [
    "afn = Afinn()\n",
    "\n",
    "def get_sentiment(text):\n",
    "    score = afn.score(text)\n",
    "    if score > 0:\n",
    "        return 'positive'\n",
    "    elif score < 0:\n",
    "        return 'negative'\n",
    "    else:\n",
    "        return 'neutral'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "81ceaecf-f45f-49e2-ae11-da721369ef90",
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_data['afinn_score'] = reddit_data['post'].apply(afn.score)\n",
    "reddit_data['sentiment'] = reddit_data['post'].apply(get_sentiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a98e7e6-e0f6-4b07-8853-e1774b479dfe",
   "metadata": {},
   "source": [
    "## Personal Pronoun Count and Ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "bf985475-f5d7-4eec-82a1-9849970747f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pronoun_count(text):\n",
    "    pronouns = re.findall(r'\\b(I|me|mine|myself|we|us|ours|ourselves)\\b', text, re.IGNORECASE)\n",
    "    return len(pronouns)\n",
    "\n",
    "reddit_data['personal_pronoun_count'] = reddit_data['post'].apply(get_pronoun_count)\n",
    "reddit_data['personal_pronoun_itra_ratio'] = reddit_data['personal_pronoun_count'] / reddit_data['personal_pronoun_count'].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dea47375-6c88-4e84-86d8-609fb65a1e92",
   "metadata": {},
   "source": [
    "## Number of Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "7969dc74-69db-419b-85c7-167d13d365e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ckhoe\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\ckhoe\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "def count_sentences(text):\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    return len(sentences)\n",
    "\n",
    "reddit_data['num_sentences'] = reddit_data['post'].apply(count_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9ecc8b6-29de-404e-9893-b826bd525e11",
   "metadata": {},
   "source": [
    "## Definite Articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "7752ed74-9d71-4b1c-9316-133e8ca46478",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_articles(text):\n",
    "    articles = re.findall(r'\\b(the)\\b', text, re.IGNORECASE)\n",
    "    return len(articles)\n",
    "\n",
    "reddit_data[\"num_articles\"] = reddit_data['post'].apply(count_articles)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c48a2c5f-aa75-41b6-a609-6b5da4c1fcd1",
   "metadata": {},
   "source": [
    "## Pronouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "75a37e55-4ba4-42c5-b9ff-1e6f20e96c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_pronouns(text):\n",
    "    sentences = nltk.word_tokenize(text)\n",
    "    tags = nltk.pos_tag(sentences)\n",
    "    pronouns = ['PRP', 'PRP$', 'WP', 'WP$']\n",
    "    \n",
    "    num_pronouns = sum([1 for word, tag in tags if tag in pronouns])\n",
    "    \n",
    "    return num_pronouns\n",
    "\n",
    "reddit_data[\"num_pronouns\"] = reddit_data['post'].apply(count_pronouns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c41d600-8a63-4492-9af1-9d69acbaa14b",
   "metadata": {},
   "source": [
    "## Tree height "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "6aa1d518-33dd-4681-9864-b2d625df71fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "snlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def tree_height(node):\n",
    "    if not list(node.children):\n",
    "        return 1\n",
    "    else:\n",
    "        return 1 + max(tree_height(child) for child in node.children)\n",
    "\n",
    "def get_tree_height(text):\n",
    "    \n",
    "    doc = snlp(text)\n",
    "    heights = [tree_height(sent.root) for sent in doc.sents]\n",
    "    \n",
    "    if not heights:\n",
    "        return None, None\n",
    "    \n",
    "    return np.mean(heights), max(heights)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "535a2982-71e0-4b63-b887-5761b735a81c",
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_data[\"mean_tree_height\"], reddit_data[\"max_tree_height\"] = zip(*reddit_data['post'].apply(get_tree_height))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf9c857b-b5a7-41c9-9604-2b6b9c40284b",
   "metadata": {},
   "source": [
    "## LabMT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "bc27deee-1172-47de-88eb-bdd498502f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_strict_match(text):\n",
    "    lang = 'english'\n",
    "    labMT, labMTvector, labMTwordList = emotionFileReader(stopval=0.0,lang=lang, returnVector=True)\n",
    "    \n",
    "    textValence, textFvec = emotion(text, labMT, shift=True, happsList=labMTvector)\n",
    "    textStoppedVec = stopper(textFvec, labMTvector, labMTwordList, stopVal=1.0)\n",
    "    textValence = emotionV(textStoppedVec, labMTvector)\n",
    "    return textValence\n",
    "\n",
    "reddit_data[\"labmt_valence\"] = reddit_data['post'].apply(get_strict_match)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fd97069-66bd-48e9-a56c-5a3135b46d13",
   "metadata": {},
   "source": [
    "## Verbal Phrase Length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "a4708524-67bb-44fd-867b-c20b83be4b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_vp_length(text):\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    max_length = 0\n",
    "    current_length = 0\n",
    "    inside_vp = False\n",
    "    \n",
    "    for token in doc:\n",
    "        if token.pos_ == \"VERB\":\n",
    "            if not inside_vp:\n",
    "                inside_vp = True\n",
    "            current_length += 1\n",
    "        elif inside_vp:\n",
    "            if token.pos_ in {\"NOUN\", \"PRON\", \"ADJ\", \"ADV\", \"ADP\", \"DET\", \"NUM\"}:\n",
    "                current_length += 1\n",
    "            else:\n",
    "                max_length = max(max_length, current_length)\n",
    "                inside_vp = False\n",
    "                current_length = 0\n",
    "                \n",
    "    max_length = max(max_length, current_length)\n",
    "    \n",
    "    return max_length\n",
    "      \n",
    "reddit_data[\"max_verb_phrase_length\"] = reddit_data['post'].apply(max_vp_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c09a7f5-31c9-451d-a1af-e515ad8b9434",
   "metadata": {},
   "source": [
    "## Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "e0a09887-2bd5-4f47-a6c5-24606a81d25c",
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_data_enriched = pd.DataFrame()\n",
    "reddit_data_enriched['user'] = reddit_data['user']\n",
    "reddit_data_enriched['post'] = reddit_data['post']\n",
    "reddit_data_enriched['label'] = reddit_data['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "a8b57c9a-2ae2-4bd4-a18f-a0d4f507f5fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(series):\n",
    "    return (series - series.mean()) / series.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "3c9d4ec0-4d88-4077-b0b5-9077f012c619",
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_data_enriched['afinn_score'] = normalize(reddit_data['afinn_score'])\n",
    "reddit_data_enriched['sentiment'] = reddit_data['sentiment']\n",
    "reddit_data_enriched = pd.get_dummies(reddit_data_enriched, columns=['sentiment'])\n",
    "reddit_data_enriched['personal_pronoun_count'] = normalize(reddit_data['personal_pronoun_count'])\n",
    "reddit_data_enriched['num_sentences'] = normalize(reddit_data['num_sentences'])\n",
    "reddit_data_enriched['num_articles'] = normalize(reddit_data['num_articles'])\n",
    "reddit_data_enriched['num_pronouns'] = normalize(reddit_data['num_pronouns'])\n",
    "reddit_data_enriched['mean_tree_height'] = normalize(reddit_data['mean_tree_height'])\n",
    "reddit_data_enriched['max_tree_height'] = normalize(reddit_data['max_tree_height'])\n",
    "reddit_data_enriched['labmt_valence'] = normalize(reddit_data['labmt_valence'])\n",
    "reddit_data_enriched['max_verb_phrase_length'] = normalize(reddit_data['max_verb_phrase_length'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "435bf1a9-8441-4d79-b905-d58c38db5aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_data_enriched.to_csv('reddit_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09a04135-8870-4a13-9029-0deb12bc5475",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
